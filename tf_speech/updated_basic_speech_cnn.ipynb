{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires running tf_speech_EDA notebook first to generate spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/tf_speech/\"\n",
    "PIC_FOLDER = \"picts\"\n",
    "audio_path = f'{PATH}train/audio/'\n",
    "pict_path = f'{PATH}{PIC_FOLDER}/train/'\n",
    "test_pict_path = f'{PATH}{PIC_FOLDER}/test/'\n",
    "test_audio_path = f'{PATH}test/audio/'\n",
    "PNG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for splitting data into validation and test sets (from README, not used yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "  \"\"\"Determines which data partition the file should belong to.\n",
    "\n",
    "  We want to keep files in the same training, validation, or testing sets even\n",
    "  if new ones are added over time. This makes it less likely that testing\n",
    "  samples will accidentally be reused in training when long runs are restarted\n",
    "  for example. To keep this stability, a hash of the filename is taken and used\n",
    "  to determine which set it should belong to. This determination only depends on\n",
    "  the name and the set proportions, so it won't change as other files are added.\n",
    "\n",
    "  It's also useful to associate particular files as related (for example words\n",
    "  spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "  ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "  'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "\n",
    "  Args:\n",
    "    filename: File path of the data sample.\n",
    "    validation_percentage: How much of the data set to use for validation.\n",
    "    testing_percentage: How much of the data set to use for testing.\n",
    "\n",
    "  Returns:\n",
    "    String, one of 'training', 'validation', or 'testing'.\n",
    "  \"\"\"\n",
    "  base_name = os.path.basename(filename)\n",
    "  # We want to ignore anything after '_nohash_' in the file name when\n",
    "  # deciding which set to put a wav in, so the data set creator has a way of\n",
    "  # grouping wavs that are close variations of each other.\n",
    "  hash_name = re.sub(r'_nohash_.*$', '', base_name)\n",
    "  # This looks a bit magical, but we need to decide whether this file should\n",
    "  # go into the training, testing, or validation sets, and we want to keep\n",
    "  # existing files in the same set even if more files are subsequently\n",
    "  # added.\n",
    "  # To do that, we need a stable way of deciding based on just the file name\n",
    "  # itself, so we do a hash of that and then use that to generate a\n",
    "  # probability value that we use to assign it.\n",
    "  hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
    "  percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                      (MAX_NUM_WAVS_PER_CLASS + 1)) *\n",
    "                     (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "  if percentage_hash < validation_percentage:\n",
    "    result = 'validation'\n",
    "  elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "    result = 'testing'\n",
    "  else:\n",
    "    result = 'training'\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load converted validation and test sets:\n",
    "(sed -i \"s/wav/png/g\" testing_list.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed/026290a7_nohash_0.png', 'bed/060cd039_nohash_0.png', 'bed/060cd039_nohash_1.png', 'bed/099d52ad_nohash_0.png', 'bed/0e17f595_nohash_0.png', 'bed/0e17f595_nohash_1.png', 'bed/105e72bb_nohash_0.png', 'bed/1657c9fa_nohash_0.png', 'bed/16db1582_nohash_0.png', 'bed/171b56dc_nohash_0.png']\n",
      "['bed/0c40e715_nohash_0.png', 'bed/0ea0e2f4_nohash_0.png', 'bed/0ea0e2f4_nohash_1.png', 'bed/105a0eea_nohash_0.png', 'bed/1528225c_nohash_0.png', 'bed/1528225c_nohash_1.png', 'bed/1528225c_nohash_2.png', 'bed/1528225c_nohash_3.png', 'bed/1b4c9b89_nohash_0.png', 'bed/1cb788bc_nohash_0.png']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6798"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"{PATH}validation_list.txt\", \"r\") as f:\n",
    "    valid_list = f.read().splitlines()\n",
    "with open(f\"{PATH}testing_list.txt\", \"r\") as f:\n",
    "    test_list = f.read().splitlines()\n",
    "print(valid_list[:10])\n",
    "print(test_list[:10])\n",
    "len(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"silence\",\"unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train list by walking through folders and checking if folder name is one of my labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left/aff582a1_nohash_1.png', 'left/5fadb538_nohash_4.png', 'left/53eb0a88_nohash_0.png', 'left/918a2473_nohash_3.png', 'left/e5dadd24_nohash_0.png', 'left/7257420c_nohash_0.png', 'left/c0e0f834_nohash_0.png', 'left/39c13eed_nohash_0.png', 'left/1daa5ada_nohash_0.png', 'left/fd395b74_nohash_4.png']\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "silence_list = []\n",
    "for x in os.listdir(pict_path):#audio_path):\n",
    "    if os.path.isdir(pict_path + x):\n",
    "        if \"silence\" in x:\n",
    "            silence_list.extend([x+\"/\"+y for y in os.listdir(pict_path + x) if '.png' in y])\n",
    "        if x not in labels:\n",
    "            train_list.extend([x+\"/\"+y for y in random.sample(os.listdir(pict_path + x), 150) if '.png' in y])\n",
    "        else:\n",
    "            train_list.extend([x+\"/\"+y for y in os.listdir(pict_path + x) if '.png' in y])\n",
    "print(train_list[:10])\n",
    "len(train_list)\n",
    "silence_list = random.sample(silence_list, len(silence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(silence_list)/100*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24719"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list = list(set(train_list)-set(valid_list))\n",
    "train_list = list(set(train_list)-set(test_list))\n",
    "train_list.extend(silence_list[:1680])\n",
    "valid_list.extend(silence_list[1680:1890])\n",
    "test_list.extend(silence_list[1890:])\n",
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/tf_speech/picts/test/clip_cec3f56cb.png',\n",
       " './data/tf_speech/picts/test/clip_fa022b6ea.png',\n",
       " './data/tf_speech/picts/test/clip_bc8564798.png',\n",
       " './data/tf_speech/picts/test/clip_c6913eb1f.png',\n",
       " './data/tf_speech/picts/test/clip_b62f28b2a.png']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_list = []\n",
    "submit_list.extend([test_pict_path+y for y in os.listdir(test_pict_path) if '.png' in y])\n",
    "submit_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_list, columns = [\"Filepath\"])\n",
    "valid_df = pd.DataFrame(valid_list, columns = [\"Filepath\"])\n",
    "test_df  = pd.DataFrame(test_list,  columns = [\"Filepath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "valid_df = valid_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn word labels into number labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yes': 0,\n",
       " 'no': 1,\n",
       " 'up': 2,\n",
       " 'down': 3,\n",
       " 'left': 4,\n",
       " 'right': 5,\n",
       " 'on': 6,\n",
       " 'off': 7,\n",
       " 'stop': 8,\n",
       " 'go': 9,\n",
       " 'silence': 10,\n",
       " 'unknown': 11}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {}\n",
    "for i, label in enumerate(labels):\n",
    "    label_dict[label] = i\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right/106a6183_nohash_0.png</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>right/be7a5b2d_nohash_3.png</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>go/742d6431_nohash_3.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>silence/pinknoise0649.png</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marvin/3fdafe25_nohash_0.png</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Filepath  Label\n",
       "0   right/106a6183_nohash_0.png      5\n",
       "1   right/be7a5b2d_nohash_3.png      5\n",
       "2      go/742d6431_nohash_3.png      9\n",
       "3     silence/pinknoise0649.png     10\n",
       "4  marvin/3fdafe25_nohash_0.png     11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_label(x):\n",
    "    label = x.split(\"/\")[0]\n",
    "    if label not in label_dict:\n",
    "        return label_dict[\"unknown\"]\n",
    "    else:\n",
    "        return label_dict[label]\n",
    "\n",
    "train_df[\"Label\"] = train_df[\"Filepath\"].apply(make_label)\n",
    "valid_df[\"Label\"] = valid_df[\"Filepath\"].apply(make_label)\n",
    "test_df[\"Label\"]  = test_df[\"Filepath\"].apply(make_label)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bed/0c40e715_nohash_0.png</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bed/0ea0e2f4_nohash_0.png</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bed/0ea0e2f4_nohash_1.png</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bed/105a0eea_nohash_0.png</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bed/1528225c_nohash_0.png</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Filepath  Label\n",
       "0  bed/0c40e715_nohash_0.png     11\n",
       "1  bed/0ea0e2f4_nohash_0.png     11\n",
       "2  bed/0ea0e2f4_nohash_1.png     11\n",
       "3  bed/105a0eea_nohash_0.png     11\n",
       "4  bed/1528225c_nohash_0.png     11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important for monitoring metrics in tensorboard\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_input_fn(files,labels, batch_size, num_epochs=1,shuffle=True):\n",
    "    \n",
    "    def _input_fn():\n",
    "        # step 1\n",
    "        #files = files.apply(lambda x: path + x)\n",
    "        filenames = tf.constant(list(files))\n",
    "        _labels = tf.constant(list(labels))\n",
    "\n",
    "        # step 2: create a dataset returning slices of `filenames`\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((filenames, _labels))\n",
    "\n",
    "        # step 3: parse every image in the dataset using `map`\n",
    "        def _parse_function(filename, label):\n",
    "            image_string = tf.read_file(filename)#tf.strings.join([path,filename])\n",
    "            image_decoded = tf.image.decode_png(image_string, channels=PNG_CHANNELS)\n",
    "            image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
    "            return image, label\n",
    "\n",
    "        dataset = dataset.map(_parse_function)\n",
    "        dataset = dataset.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "        if shuffle:\n",
    "          dataset = dataset.shuffle(200)\n",
    "        \n",
    "        # step 4: create iterator and final input tensor\n",
    "        image_batch, label_batch = dataset.make_one_shot_iterator().get_next()\n",
    "        \n",
    "        #tf.Print(label_batch,[label_batch],message=\"Image: \")\n",
    "        \n",
    "        return image_batch, label_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predict_input_fn(files, labels, batch_size):\n",
    "    \n",
    "    def _input_fn():\n",
    "        # step 1\n",
    "        filenames = tf.constant(list(files))\n",
    "        _labels = tf.constant(list(labels))\n",
    "\n",
    "        # step 2: create a dataset returning slices of `filenames`\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((filenames, _labels))\n",
    "\n",
    "        # step 3: parse every image in the dataset using `map`\n",
    "        def _parse_function(filename, label):\n",
    "            image_string = tf.read_file(filename)#tf.strings.join([path,filename])\n",
    "            image_decoded = tf.image.decode_png(image_string, channels=PNG_CHANNELS)\n",
    "            image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
    "            return image, label\n",
    "\n",
    "        dataset = dataset.map(_parse_function)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        # step 4: create iterator and final input tensor\n",
    "        image_batch, label_batch = dataset.make_one_shot_iterator().get_next()\n",
    "        \n",
    "        return image_batch, label_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submit_input_fn(files, batch_size):\n",
    "    \n",
    "    def _input_fn():\n",
    "        # step 1\n",
    "        filenames = tf.constant(list(files))\n",
    "        #_labels = tf.constant(list(labels))\n",
    "\n",
    "        # step 2: create a dataset returning slices of `filenames`\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "\n",
    "        # step 3: parse every image in the dataset using `map`\n",
    "        def _parse_function(filename):\n",
    "            image_string = tf.read_file(filename)#tf.strings.join([path,filename])\n",
    "            image_decoded = tf.image.decode_png(image_string, channels=PNG_CHANNELS)\n",
    "            image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
    "            return image\n",
    "\n",
    "        dataset = dataset.map(_parse_function)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        # step 4: create iterator and final input tensor\n",
    "        image_batch = dataset.make_one_shot_iterator().get_next()\n",
    "        \n",
    "        return image_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "IMG_SIZE = (256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = create_train_input_fn(train_df[\"Filepath\"].apply(lambda x: pict_path + x),train_df[\"Label\"],\n",
    "                                       batch_size=BATCH_SIZE)\n",
    "valid_input_fn = create_predict_input_fn(valid_df[\"Filepath\"].apply(lambda x: pict_path + x),valid_df[\"Label\"],\n",
    "                                       batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "test_input_fn = create_predict_input_fn(test_df[\"Filepath\"].apply(lambda x: pict_path + x),test_df[\"Label\"],\n",
    "                                       batch_size=BATCH_SIZE)\n",
    "submit_input_fn = create_submit_input_fn(submit_list, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hooks = [tf_debug.LocalCLIDebugHook()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_tensor, depth, kernel, name, strides=(1, 1), padding=\"VALID\"):\n",
    "    return tf.layers.conv2d(input_tensor, filters=depth, kernel_size=kernel, strides=strides, padding=padding, activation=tf.nn.leaky_relu, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(features, labels, mode):\n",
    "    net = tf.reshape(features, [-1,IMG_SIZE[0],IMG_SIZE[1],PNG_CHANNELS])\n",
    "    net = conv2d(net,16,[3,3],\"conv1\",(1,1)) #254,126,16\n",
    "    net = tf.layers.max_pooling2d(net, 2,2) # 127,63,16\n",
    "    net = conv2d(net,32,[3,3],\"conv2\",(2,2)) #63,31,32\n",
    "    net = tf.layers.max_pooling2d(net, 2,1)# 62,30,32   \n",
    "    net = conv2d(net,64,[2,2],\"conv3\",(2,2)) #31,15,64\n",
    "    net = tf.layers.max_pooling2d(net, 2,1)# 30,14,64\n",
    "    net = conv2d(net,128,[2,2],\"conv4\",(2,2))#15,7,128\n",
    "    net = tf.layers.max_pooling2d(net, 2,1)#14,6,128\n",
    "    net = conv2d(net,256,[2,2],\"conv5\", (2,2))# 7,3,256\n",
    "    net = tf.layers.flatten(net)\n",
    "    net = tf.layers.dense(net, units = 256, activation=tf.nn.leaky_relu)\n",
    "    net = tf.layers.dropout(net, rate = 0.2, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "    logits = tf.layers.dense(net,units = 12)\n",
    "\n",
    "    predictions = {\n",
    "                    \"classes\": tf.argmax(input=logits, axis=1),\n",
    "                    \"probabilities\": tf.nn.softmax(logits)\n",
    "    }\n",
    "    \n",
    "    #define predict method logic\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, predictions=predictions)\n",
    "    \n",
    "    #sparse_softmax does one-hot automatically\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits = logits)\n",
    "    accuracy = tf.metrics.accuracy(labels = labels, predictions = predictions[\"classes\"], name = \"acc_op\")\n",
    "    \n",
    "    \n",
    "    #define train method logic\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "        train_op  = optimizer.minimize(\n",
    "            loss = loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        tf.identity(accuracy[1], name='train_accuracy')\n",
    "        tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "        eval_metric_ops = {\"train_accuracy\":accuracy}\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, loss = loss, train_op = train_op, eval_metric_ops=eval_metric_ops)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #define evaluate method logic\n",
    "    tf.identity(accuracy[1], name='val_accuracy')\n",
    "    tf.summary.scalar('val_accuracy', accuracy[1])\n",
    "    eval_metric_ops = {\"val_accuracy\":accuracy}\n",
    "    return tf.estimator.EstimatorSpec(mode = mode, loss = loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tf_files\n",
    "OUT_DIR = \"./tf_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './tf_files', '_tf_random_seed': None, '_save_summary_steps': 2, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f80d2360eb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.474491, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 100 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.6078167.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:34:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:34:49\n",
      "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 2.1828713, val_accuracy = 0.13199201\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: ./tf_files/model.ckpt-100\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 100 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.8275142, step = 100\n",
      "INFO:tensorflow:Saving checkpoints for 200 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.5178158.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:35:25\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:35:37\n",
      "INFO:tensorflow:Saving dict for global step 200: global_step = 200, loss = 1.9916258, val_accuracy = 0.24828768\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: ./tf_files/model.ckpt-200\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 200 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.5940864, step = 200\n",
      "INFO:tensorflow:Saving checkpoints for 300 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.1314263.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:36:13\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:36:24\n",
      "INFO:tensorflow:Saving dict for global step 300: global_step = 300, loss = 1.8118377, val_accuracy = 0.30465183\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 300: ./tf_files/model.ckpt-300\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 300 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.4026859, step = 300\n",
      "INFO:tensorflow:Saving checkpoints for 400 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.0209837.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:37:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:37:12\n",
      "INFO:tensorflow:Saving dict for global step 400: global_step = 400, loss = 1.4940274, val_accuracy = 0.4250856\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: ./tf_files/model.ckpt-400\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 400 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.8849071, step = 400\n",
      "INFO:tensorflow:Saving checkpoints for 500 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.092359.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:37:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:38:00\n",
      "INFO:tensorflow:Saving dict for global step 500: global_step = 500, loss = 1.4000782, val_accuracy = 0.48715752\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: ./tf_files/model.ckpt-500\n"
     ]
    }
   ],
   "source": [
    "my_cnn_classifier = tf.estimator.Estimator(model_fn=conv_model, \n",
    "                                           config=tf.estimator.RunConfig(keep_checkpoint_max=1).replace(save_summary_steps=2),\n",
    "                                           model_dir=OUT_DIR)\n",
    "file_writer = tf.summary.FileWriter(OUT_DIR)\n",
    "\n",
    "def train_and_eval(estimator, num_epochs = NUM_EPOCHS, steps=100):\n",
    "    for n in range(num_epochs):\n",
    "        estimator.train(input_fn = train_input_fn,steps = steps)\n",
    "        estimator.evaluate(input_fn = valid_input_fn)\n",
    "        \n",
    "train_and_eval(my_cnn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 500 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.8912085, step = 500\n",
      "INFO:tensorflow:Saving checkpoints for 600 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.9930417.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:38:36\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:38:48\n",
      "INFO:tensorflow:Saving dict for global step 600: global_step = 600, loss = 1.2341361, val_accuracy = 0.5542237\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 600: ./tf_files/model.ckpt-600\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 600 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.85157996, step = 600\n",
      "INFO:tensorflow:Saving checkpoints for 700 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.777823.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:39:24\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:39:36\n",
      "INFO:tensorflow:Saving dict for global step 700: global_step = 700, loss = 1.061908, val_accuracy = 0.6104452\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 700: ./tf_files/model.ckpt-700\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 700 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7813796, step = 700\n",
      "INFO:tensorflow:Saving checkpoints for 800 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.7609508.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:40:12\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:40:23\n",
      "INFO:tensorflow:Saving dict for global step 800: global_step = 800, loss = 1.0063244, val_accuracy = 0.6348459\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 800: ./tf_files/model.ckpt-800\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 800 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.44660413, step = 800\n",
      "INFO:tensorflow:Saving checkpoints for 900 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5164977.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:41:00\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-900\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:41:11\n",
      "INFO:tensorflow:Saving dict for global step 900: global_step = 900, loss = 0.8897102, val_accuracy = 0.6827911\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 900: ./tf_files/model.ckpt-900\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-900\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 900 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.44027364, step = 900\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./tf_files/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.43421215.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-27-02:41:47\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-27-02:41:59\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.774884, val_accuracy = 0.7216039\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: ./tf_files/model.ckpt-1000\n"
     ]
    }
   ],
   "source": [
    "train_and_eval(my_cnn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission file generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf_files/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "generator = my_cnn_classifier.predict(input_fn=submit_input_fn)\n",
    "predictions = [next(generator) for i in range(len(submit_list))]\n",
    "classes = [predictions[i][\"classes\"] for i in range(len(predictions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission_file(classes, filename):\n",
    "    submission = pd.DataFrame()\n",
    "    submission[\"fname\"] = submit_list\n",
    "    submission[\"fname\"] = submission[\"fname\"].apply(lambda x: x.split(\"/\")[-1][:-3]+\"wav\")\n",
    "    submission[\"label\"] = classes\n",
    "    submission[\"label\"] = submission[\"label\"].apply(lambda x: labels[x])\n",
    "    submission.set_index(\"fname\", inplace=True)\n",
    "    submission.to_csv(filename)\n",
    "    \n",
    "make_submission_file(classes, \"tf_speech_pred_cnn.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
